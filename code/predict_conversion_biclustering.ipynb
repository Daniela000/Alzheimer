{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lasige.di.fc.ul.pt/damaral/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import set_config\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, GridSearchCV, LeaveOneOut\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.naive_bayes import GaussianNB, ComplementNB\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import SpectralBiclustering\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"bic_prediction.log\", \"w\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/lisbon/conversion_ad/2tw_no_norm_all.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tw \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlisbon\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/conversion_ad/\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43mtw_no_norm_all.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m all_features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvolution\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/lisbon/conversion_ad/2tw_no_norm_all.csv'"
     ]
    }
   ],
   "source": [
    "tw = 2\n",
    "dir ='lisbon'\n",
    "df = pd.read_csv('../data/'+dir+'/conversion_ad/{}tw_no_norm_all.csv'.format(tw))\n",
    "all_features = df.copy()\n",
    "y = df['Evolution'].copy()\n",
    "all_features.drop(columns = ['Code','Group', 'BBA', 'CSFdate', 'comentarios', 'Conversion', 'data', 'tempofollowup', 'Evolution', 'Cluster', 'sexo', 'idade', 'CSFdatatTau', 'CSFdatapTau', 'CSFdataabeta42', 'MMSE', 'uguL', 'Volumefor40ug'], inplace = True) # drop unwanted columns\n",
    "#all_features['sexo'].replace({'M' : 0, 'F': 1}, inplace = True)\n",
    "#numerical_features = [feature for feature in all_features.columns if feature not in ['sexo']]\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifiers to evaluate\n",
    "# Define classifiers and their parameter grids\n",
    "classifiers = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(solver='liblinear'),\n",
    "        'params': {\n",
    "            'classification__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'classification__class_weight' : [None, 'balanced'],\n",
    "            'classification__penalty': ['l1', 'l2'], 'classification__random_state' : [42]\n",
    "        }\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classification__n_estimators': [10,50, 100, 200],\n",
    "            'classification__max_depth': [None, 10, 15,20],\n",
    "            'classification__max_features' : ['sqrt', 'log2',None],\n",
    "            'classification__class_weight' : [None, 'balanced'], 'classification__random_state' : [42]\n",
    "        }\n",
    "    },\n",
    "    'XGBgClassifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'classification__n_estimators': [10, 50, 100,300],\n",
    "            'classification__max_depth': [5,10,20,30],\n",
    "            'classification__learning_rate': [0.0001, 0.001, 0.01, 0.1,1],\n",
    "            'classification__objective': ['binary:logistic'], 'classification__random_state' : [42]\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'classification__C': [0.1, 1, 10, 5],\n",
    "            'classification__kernel': ['linear','poly','rbf', 'sigmoid'],\n",
    "            'classification__probability': [True],\n",
    "            'classification__class_weight' : [None, 'balanced'], 'classification__random_state' : [42]\n",
    "        }\n",
    "    },\n",
    "    'NB': {\n",
    "        'model': GaussianNB(),\n",
    "        'params': {'classification__var_smoothing' : [10**-9, 10**-8, 10**-10]}\n",
    "    },\n",
    "    'DTClassifier': {\n",
    "    'model': tree.DecisionTreeClassifier(),\n",
    "    'params': {'classification__max_depth': [None, 10, 20],\n",
    "              'classification__class_weight' : [None, 'balanced'],\n",
    "              'classification__random_state' : [42]}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n",
    "def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n",
    "def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n",
    "def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n",
    "\n",
    "\n",
    "def sens(y_true, y_pred): return tp(y_true, y_pred) / \\\n",
    "    (fn(y_true, y_pred) + tp(y_true, y_pred))\n",
    "\n",
    "\n",
    "def spec(y_true, y_pred): return tn(y_true, y_pred) / \\\n",
    "    (fp(y_true, y_pred) + tn(y_true, y_pred))\n",
    "\n",
    "sensitivity_scorer = make_scorer(sens)\n",
    "\n",
    "# Specificity scorer\n",
    "specificity_scorer = make_scorer(spec)\n",
    "\n",
    "# AUC scorer\n",
    "auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "# accuracy scorer\n",
    "accuracy_scorer = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Biclustering(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, n_clusters_pos = 3, method_pos = 'bistochastic',svd_method_pos = 'randomized', n_clusters_neg = 3, method_neg = 'bistochastic',svd_method_neg = 'randomized', random_state=42):\n",
    "        self.n_clusters_pos = n_clusters_pos\n",
    "        self.method_pos = method_pos\n",
    "        self.svd_method_pos = svd_method_pos\n",
    "        self.n_clusters_neg = n_clusters_neg\n",
    "        self.method_neg = method_neg\n",
    "        self.svd_method_neg = svd_method_neg\n",
    "        self.random_state = random_state\n",
    "        self.biclusters = []\n",
    "    def fit(self, X, y):\n",
    "        # Clear biclusters to prevent accumulation\n",
    "        self.biclusters = []\n",
    "        self.x_train = X\n",
    "        self.biclustering = SpectralBiclustering(\n",
    "            n_clusters=self.n_clusters_pos, \n",
    "            method=self.method_pos, \n",
    "            svd_method=self.svd_method_pos, \n",
    "            random_state=self.random_state)\n",
    "        \n",
    "        #biclusters of the positive class\n",
    "        positive_indices = [i for i, val in enumerate(y) if val > 0]\n",
    "        self.biclustering.fit(X.loc[positive_indices, :])\n",
    "        positive_biclusters = self.post_processing_bicluster(X[positive_indices, :])\n",
    "        positive_biclusters = self.filter_trivial(positive_biclusters)\n",
    "\n",
    "\n",
    "        self.biclustering = SpectralBiclustering(\n",
    "            n_clusters=self.n_clusters_neg, \n",
    "            method=self.method_neg, \n",
    "            svd_method=self.svd_method_neg, \n",
    "            random_state=self.random_state)\n",
    "\n",
    "        # biclusters of the negative class\n",
    "        negative_indices = [i for i, val in enumerate(y) if val == 0]\n",
    "        self.biclustering.fit(X.loc[negative_indices, :])\n",
    "        negative_indices = self.post_processing_bicluster(X[negative_indices, :])\n",
    "        negative_indices = self.filter_trivial(negative_indices)\n",
    "        # concat biclusters\n",
    "        self.biclusters = positive_biclusters + negative_indices\n",
    "        print(self.biclusters)\n",
    "        # Check if biclusters were created\n",
    "        if len(self.biclusters) == 0:\n",
    "            print(\"Warning: No biclusters were found.\")\n",
    "        else:\n",
    "            print(f\"Found {len(self.biclusters)} biclusters.\")\n",
    "        return self\n",
    "\n",
    "    def get_number_bics(self):\n",
    "        return len(self.biclusters)\n",
    "\n",
    "    def filter_trivial(self, biclusters):\n",
    "        print(biclusters)\n",
    "        return [bic for bic in biclusters if len(bic[0]) >= 2 and len(bic[1]) >= 2]\n",
    "        #print(self.biclusters)\n",
    "\n",
    "    def post_processing_bicluster(self,X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        # Extracting the biclusters\n",
    "        n_biclusters = max(list(self.biclustering.row_labels_) + list(self.biclustering.column_labels_)) + 1\n",
    "        rows = defaultdict(list)\n",
    "        cols = defaultdict(list)\n",
    "\n",
    "        row_labels = self.biclustering.row_labels_\n",
    "        col_labels = self.biclustering.column_labels_\n",
    "\n",
    "        for i, label in enumerate(row_labels):\n",
    "           rows[label].append(X.index[i])\n",
    "\n",
    "        for j, label in enumerate(col_labels):\n",
    "            cols[label].append(X.columns[j])\n",
    "        \n",
    "        # Build biclusters\n",
    "        return [[rows[i], cols[i]] for i in range(n_biclusters)]\n",
    "        \n",
    "        \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "        # Initialize dictionary for distances\n",
    "\n",
    "        distance_matrix = []\n",
    "        if len(self.biclusters) == 0:\n",
    "            print('Warning: No biclusters found, returning zero matrix')\n",
    "            return np.zeros(((X.shape[0], 1)))\n",
    "        #col_name = []\n",
    "        for b in range(len(self.biclusters)):\n",
    "            P = self.x_train.loc[self.biclusters[b][0],self.biclusters[b][1]] #patterns in the train set\n",
    "            # Check if P is empty or has NaNs\n",
    "            if P.empty or P.isnull().values.any():\n",
    "                continue\n",
    "            y_labels = P.columns\n",
    "            P = P.mean(axis = 0)\n",
    "            if len(P) == 0:\n",
    "                continue\n",
    "            # Precompute row submatrix \n",
    "            X_submatrix = X.loc[:, y_labels]\n",
    "            # Make sure shapes match before subtraction\n",
    "            if X_submatrix.shape[1] != len(P):\n",
    "                continue  # Skip if shapes are incompatible\n",
    "            # Compute the distance for each row\n",
    "            diff = (X_submatrix - P) ** 2\n",
    "            distance_matrix.append(np.sqrt(diff.sum(axis=1)) / len(P))\n",
    "            #col_name.append('bic_' + str(b)) \n",
    "\n",
    "        if len(distance_matrix) == 0:\n",
    "            print(\"Warning: No valid distances computed, returning zero matrix\")\n",
    "            return np.zeros(((X.shape[0], 1)))\n",
    "        distance_matrix = np.array(distance_matrix).T\n",
    "        \n",
    "        if len(distance_matrix[1]) == 1: #if one feature\n",
    "            distance_matrix = distance_matrix.reshape(-1, 1)\n",
    "        #distance_matrix = pd.DataFrame(distance_matrix, index = col_name).T\n",
    "        \n",
    "        return distance_matrix\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit(X,y).transform(X)\n",
    "    \n",
    "    def get_biclusters(self):\n",
    "        # Initialize an empty list to store each bicluster's data as a row\n",
    "        bicluster_rows = []\n",
    "        \n",
    "        for b in range(len(self.biclusters)):\n",
    "            # Dictionary to hold data for the current bicluster\n",
    "            bicluster_data = {'ID': b, 'Pattern': [], 'n_samples': []}\n",
    "\n",
    "            # Loop through the features (columns) for this bicluster\n",
    "            for feature in self.biclusters[b][1]:\n",
    "                # Get the data for the rows and current feature in the bicluster\n",
    "                feature_values = self.x_train.loc[self.biclusters[b][0], feature]\n",
    "                \n",
    "                # Calculate the min and max values\n",
    "                min_val = feature_values.min()\n",
    "                max_val = feature_values.max()\n",
    "                \n",
    "                # Append the pattern in the format 'feature = [min, max]'\n",
    "                bicluster_data['Pattern'].append(f'{feature} = [{min_val:.2f}, {max_val:.2f}]')\n",
    "            bicluster_data['n_samples'].append(len(feature_values))\n",
    "            # Append the bicluster data to the list of rows\n",
    "            bicluster_rows.append(bicluster_data)\n",
    "        \n",
    "        # Convert the list of bicluster data into a DataFrame\n",
    "        bicluster_table = pd.DataFrame(bicluster_rows)\n",
    "        \n",
    "        # Display the table\n",
    "        print(bicluster_table)\n",
    "        return bicluster_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Define the cross-validation procedure (5-fold cross-validation with 10 repetitions)\n",
    "#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "cv = LeaveOneOut()\n",
    "all_scores = []\n",
    "k_scores = []\n",
    "# List to store results\n",
    "results = []\n",
    "#print(all_features)\n",
    "set_config(transform_output=\"pandas\")\n",
    "# Define the ColumnTransformer to apply MinMaxScaler only to numerical columns\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#        ('num', MinMaxScaler(), numerical_features),  # Custom scaler for numerical features\n",
    "#        ('cat', 'passthrough', ['sexo'])            # Pass categorical columns unchanged\n",
    "#    ], remainder='passthrough', verbose_feature_names_out=False, sparse_threshold=0)     #remainder='passthrough'  # Ensure other columns are passed through if not specified\n",
    "\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(name)\n",
    "    pipeline = ImbPipeline([#('scaler', preprocessor),\n",
    "                            ('biclustering', Biclustering()),\n",
    "                            ('classification', clf['model'])])\n",
    "    n_pos = sum(y)\n",
    "    n_neg = len(y) - sum(y)\n",
    "    bic_params = {'biclustering__n_clusters_pos' : list(range(2,n_pos)),\n",
    "                  'biclustering__n_clusters_neg' : list(range(1,n_neg)),\n",
    "                  'biclustering__method_pos' : ['bistochastic', 'scale', 'log'],\n",
    "                  'biclustering__svd_method_pos' : ['randomized', 'arpack'],\n",
    "                  'biclustering__method_neg' : ['bistochastic', 'scale', 'log'],\n",
    "                  'biclustering__svd_method_neg' : ['randomized', 'arpack']}\n",
    "    \n",
    "    clf['params'].update(bic_params)  \n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(pipeline, clf['params'], cv=cv, scoring={'AUC': auc_scorer, 'Sensitivity': sensitivity_scorer, 'Specificity': specificity_scorer, 'Accuracy': accuracy_scorer}, refit = 'AUC', n_jobs=-1)\n",
    "    grid_search.fit(all_features, y)\n",
    "    # Get the index of the best model (based on AUC score)\n",
    "    best_index = grid_search.best_index_\n",
    "     # Extract the sensitivity and specificity for the best model\n",
    "    best_sensitivity = grid_search.cv_results_['mean_test_Sensitivity'][best_index]\n",
    "    best_std_sensitivity = grid_search.cv_results_['std_test_Sensitivity'][best_index]\n",
    "    best_specificity = grid_search.cv_results_['mean_test_Specificity'][best_index]\n",
    "    best_std_specificity = grid_search.cv_results_['std_test_Specificity'][best_index]\n",
    "    best_accuracy = grid_search.cv_results_['mean_test_Accuracy'][best_index]\n",
    "    best_std_accuracy = grid_search.cv_results_['std_test_Accuracy'][best_index]\n",
    "    # Get the best estimator (pipeline) from the grid search\n",
    "    best_model = grid_search.best_estimator_\n",
    "   \n",
    "    # Store results\n",
    "    results.append({\n",
    "    'classifier': name,\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'best_auc': grid_search.best_score_,\n",
    "    'best_std_auc' :  grid_search.cv_results_['std_test_AUC'][best_index],\n",
    "    'best_sensitivity': best_sensitivity,\n",
    "    'best_std_sensitivity': best_std_sensitivity,\n",
    "    'best_specificity': best_specificity,\n",
    "    'best_std_specificity': best_std_specificity,\n",
    "    'best_accuracy': best_accuracy,\n",
    "    'best_std_accuracy': best_std_accuracy,\n",
    "    })\n",
    "    print('best_auc :',  grid_search.best_score_)\n",
    "\n",
    "# Find the best classifier based on accuracy\n",
    "best_result = max(results, key=lambda x: x['best_auc'])\n",
    "best = f\"Best Classifier: {best_result['classifier']} \\n\" + \\\n",
    "f\"Best Params: {best_result['best_params']}\" +\"\\n\" + f\"Best AUC: {best_result['best_auc']:.4f}\" +\"\\n\" + f\"Best Sens: {best_result['best_sensitivity']:.4f}\" +\"\\n\"+ \\\n",
    "f\"Best Spec: {best_result['best_specificity']:.4f}\" + \"\\n\"  + f\"Best Accuracy: {best_result['best_accuracy']:.4f}\" + \"\\n\"\n",
    "\n",
    "\n",
    "# Create the LaTeX table as a string\n",
    "latex_table = f\"\"\"\n",
    "\\\\begin{{table}}[htbp]\n",
    "\\\\centering\n",
    "\\\\caption{{Best Classifier Performance and Biclustering {tw}tw}}\n",
    "\\\\begin{{tabular}}{{|l|l|}}\n",
    "\\\\hline\n",
    "\\\\textbf{{Best Classifier}} & {best_result['classifier']} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Classifier Parameters}} & {best_result['best_params']} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Best AUC}} & {best_result['best_auc']:.2f} $\\\\pm$ {best_result['best_std_auc']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Best Sensitivity}} & {best_result['best_sensitivity']:.2f} $\\\\pm$ {best_result['best_std_sensitivity']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Best Specificity}} & {best_result['best_specificity']:.2f} $\\\\pm$ {best_result['best_std_specificity']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\textbf{{Best Accuracy}} & {best_result['best_accuracy']:.2f} $\\\\pm$ {best_result['best_std_accuracy']:.2f} \\\\\\\\\n",
    "\\\\hline\n",
    "\\\\end{{tabular}}\n",
    "\\\\label{{tab:best_classifier}}\n",
    "\\\\end{{table}}\n",
    "\"\"\"\n",
    "print(best)\n",
    "#print(latex_table)\n",
    "\n",
    "with open(f'{tw}tw_biclustering.txt', 'w') as f:\n",
    "    f.write(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set.loc[:, 'sexo'] = train_set['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test.loc[:, 'sexo'] = X_test['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 biclusters.\n",
      "    ID                                            Pattern n_samples\n",
      "0    0  [CSFdataabeta42 = [0.24, 0.50], SLIK4 = [0.09,...       [3]\n",
      "1    1  [HRG = [0.29, 1.00], CERU = [0.59, 1.00], FHR1...       [3]\n",
      "2    2  [ALS = [0.44, 0.45], SE6L1 = [0.67, 0.72], CO9...       [5]\n",
      "3    3  [ZA2G = [0.15, 0.15], LTBP1 = [1.00, 1.00], FS...       [2]\n",
      "4    4  [C1QT1 = [0.58, 0.60], TPP1 = [0.18, 0.19], C1...       [2]\n",
      "5    5  [OMD = [0.02, 0.53], SCG1 = [0.13, 0.27], MA1C...       [2]\n",
      "6    6  [PLXB2 = [0.04, 0.15], GRIA4 = [0.21, 1.00], P...       [2]\n",
      "7    7  [idade = [0.67, 0.67], MMSE = [0.11, 0.11], AP...       [2]\n",
      "8    8         [CLC11 = [0.06, 0.06], VWF = [0.04, 0.04]]       [2]\n",
      "9    9  [C1RL = [0.10, 0.27], FHR2 = [0.06, 0.10], CHR...       [2]\n",
      "10  10  [NEO1 = [0.18, 0.61], VGF = [0.11, 0.30], GLU2...       [3]\n",
      "AUC: 0.39560439560439564 \n",
      " Sensitivity: 0.0 \n",
      " Specificity: 1.0 \n",
      " Accuracy: 0.35 \n",
      "\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set.loc[:, 'sexo'] = train_set['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test.loc[:, 'sexo'] = X_test['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 biclusters.\n",
      "    ID                                            Pattern n_samples\n",
      "0    0  [PLXB2 = [0.30, 1.00], ALS = [0.46, 1.00], PON...       [3]\n",
      "1    1  [ZA2G = [0.07, 0.18], CO6 = [0.07, 0.50], ACTS...       [2]\n",
      "2    2  [A1AG1 = [0.25, 0.26], ANT3 = [0.79, 0.80], OM...       [2]\n",
      "3    3  [LFNG = [0.06, 0.07], HBB = [0.31, 0.32], HBA ...       [2]\n",
      "4    4  [KCC2A = [0.18, 0.64], PEBP1 = [0.40, 0.68], P...       [4]\n",
      "5    5  [IBP7 = [0.66, 0.87], GLU2B = [0.38, 0.40], SA...       [2]\n",
      "6    6  [KV37 = [0.31, 0.54], A1BG = [0.64, 0.65], GDI...       [2]\n",
      "7    7  [sexo = [0.00, 1.00], GRIA4 = [0.08, 0.53], SP...       [4]\n",
      "8    8  [COCA1 = [0.00, 0.61], KLKB1 = [0.03, 0.49], C...       [4]\n",
      "9    9  [A2GL = [0.25, 0.53], NID1 = [0.26, 0.43], EGF...       [3]\n",
      "10  10  [APOA1 = [0.54, 0.78], APOA4 = [0.74, 1.00], A...       [2]\n",
      "AUC: 0.421875 \n",
      " Sensitivity: 0.3125 \n",
      " Specificity: 0.75 \n",
      " Accuracy: 0.4 \n",
      "\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set.loc[:, 'sexo'] = train_set['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test.loc[:, 'sexo'] = X_test['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 biclusters.\n",
      "    ID                                            Pattern n_samples\n",
      "0    0  [NID1 = [0.44, 0.48], DCC = [0.05, 0.09], EFNB...       [2]\n",
      "1    1  [ANT3 = [0.78, 0.87], ICAM5 = [0.19, 0.61], KN...       [2]\n",
      "2    2  [LFNG = [0.08, 0.08], FSTL5 = [0.75, 0.79], FR...       [2]\n",
      "3    3  [ZA2G = [0.44, 0.44], LTBP1 = [0.35, 0.35], DH...       [2]\n",
      "4    4  [COMP = [0.16, 0.64], PLXB2 = [0.33, 0.43], AL...       [2]\n",
      "5    5  [MMSE = [0.44, 0.44], OMD = [0.16, 0.19], KV10...       [2]\n",
      "6    6  [APOA4 = [0.32, 0.45], TTHY = [0.19, 0.33], PG...       [2]\n",
      "7    7  [CSFdataabeta42 = [0.05, 0.07], EGFLA = [0.09,...       [2]\n",
      "8    8  [GRIA4 = [0.58, 0.88], C1QT1 = [0.35, 0.43], C...       [2]\n",
      "9    9  [CERU = [0.31, 0.62], HEMO = [0.21, 0.52], A1B...       [2]\n",
      "10  10  [FETUA = [0.43, 0.58], SORL = [0.13, 0.41], C1...       [2]\n",
      "AUC: 0.6111111111111112 \n",
      " Sensitivity: 1.0 \n",
      " Specificity: 0.0 \n",
      " Accuracy: 0.9 \n",
      "\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_set.loc[:, 'sexo'] = train_set['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:13: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X_test.loc[:, 'sexo'] = X_test['sexo'].replace({'M' : 0, 'F': 1})\n",
      "C:\\Users\\danis\\AppData\\Local\\Temp\\ipykernel_11916\\3977669426.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 biclusters.\n",
      "   ID                                            Pattern n_samples\n",
      "0   0  [ANT3 = [0.34, 0.40], CERU = [0.00, 0.63], CBP...       [5]\n",
      "1   1  [APOL1 = [0.08, 0.49], ZA2G = [0.16, 0.22], PL...       [3]\n",
      "2   2  [VGF = [0.20, 0.22], ACTS = [0.83, 0.84], DSG2...       [3]\n",
      "3   3  [idade = [0.05, 0.67], MMSE = [0.11, 0.44], NR...       [3]\n",
      "4   4  [KV106 = [0.24, 0.24], CBLN4 = [0.49, 0.49], F...       [2]\n",
      "5   5  [APOA4 = [0.74, 1.00], COCA1 = [0.34, 0.38], E...       [2]\n",
      "6   6  [IBP6 = [0.99, 1.00], MDHC = [0.76, 0.77], CBP...       [2]\n",
      "7   7        [CLC11 = [0.03, 0.35], ALBU = [0.43, 0.96]]       [4]\n",
      "8   8  [A2GL = [0.37, 1.00], SLIK4 = [0.33, 0.58], CA...       [2]\n",
      "AUC: 0.4473684210526315 \n",
      " Sensitivity: 0.8947368421052632 \n",
      " Specificity: 0.0 \n",
      " Accuracy: 0.85 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "set_config(transform_output=\"pandas\")\n",
    "for k in range(2,6):\n",
    "    print(k)\n",
    "    train_set = pd.read_csv('../data/lisbon/conversion_ad/{}tw_no_norm.csv'.format(k))\n",
    "    y_train = train_set['Evolution'].copy()\n",
    "    train_set.drop(columns = ['Code','Group', 'BBA', 'CSFdate', 'comentarios', 'Conversion', 'data', 'tempofollowup', 'Evolution', 'Cluster'], inplace = True) # drop unwanted columns\n",
    "    train_set.loc[:, 'sexo'] = train_set['sexo'].replace({'M' : 0, 'F': 1})\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(train_set, y_train, test_size=0.2, random_state=42)\n",
    "    X_train = train_set\t\n",
    "    test_set = pd.read_csv('../data/coimbra/conversion_ad/{}tw_no_norm.csv'.format(k))\n",
    "    y_test = test_set['Evolution'].copy()\n",
    "    X_test = test_set[train_set.columns]\n",
    "    X_test.loc[:, 'sexo'] = X_test['sexo'].replace({'M' : 0, 'F': 1})\n",
    "    numerical_features = [i for i in X_train.columns if i !='Sexo']\n",
    "\n",
    "    #test_set['sexo'].replace({'M' : 0, 'F': 1}, inplace = True)\n",
    "\n",
    "    if k == 2:\n",
    "        biclustering = Biclustering(method='bistochastic', n_clusters = 32, svd_method='arpack')\n",
    "        clf = RandomForestClassifier(class_weight=None, max_depth = None, max_features = 'log2', n_estimators = 50, random_state=42)\n",
    "        k_neighbors = 5\n",
    "    elif k == 3:\n",
    "        biclustering = Biclustering(method='bistochastic', n_clusters = 20, svd_method='randomized')\n",
    "        clf = XGBClassifier(learning_rate=1, max_depth = 5,n_estimators = 300, objective = 'binary:logistic', random_state=42)\n",
    "        k_neighbors = 5\n",
    "    elif k == 4: \n",
    "        biclustering = Biclustering(method='log', n_clusters = 33, svd_method='randomized')\n",
    "        clf = XGBClassifier(learning_rate=1, max_depth = 5,n_estimators = 50, objective = 'binary:logistic', random_state=42)\n",
    "        k_neighbors = 5\n",
    "    elif k == 5:\n",
    "        biclustering = Biclustering(method='log', n_clusters = 32, svd_method='arpack')\n",
    "        clf = RandomForestClassifier(class_weight=None, max_depth = 10, max_features = 'log2', n_estimators = 10, random_state=42)\n",
    "        k_neighbors = 3\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train[numerical_features]= scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "    X_train = biclustering.fit_transform(X_train)\n",
    "    X_test = biclustering.transform(X_test)\n",
    "    bicluster_table = biclustering.get_biclusters()\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])\n",
    "    sensitivity = sens(y_test, y_pred)\n",
    "    specificity = spec(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    results_str = f'AUC: {auc} \\n Sensitivity: {sensitivity} \\n Specificity: {specificity} \\n Accuracy: {accuracy} \\n'\n",
    "    print(results_str)\n",
    "    latex_table = f\"\"\"\n",
    "        \\\\begin{{table}}[htbp]\n",
    "        \\\\centering\n",
    "        \\\\caption{{Best Classifier Performance and Biclustering {k}tw}}\n",
    "        \\\\begin{{tabular}}{{|l|l|}}\n",
    "        \\\\hline\n",
    "        \\\\textbf{{AUC}} & {auc:.2f}\\\\\\\\\n",
    "        \\\\hline\n",
    "        \\\\textbf{{Sensitivity}} & {sensitivity:.2f} \\\\\\\\\n",
    "        \\\\hline\n",
    "        \\\\textbf{{Specificity}} & {specificity:.2f} \\\\\\\\\n",
    "        \\\\hline\n",
    "        \\\\textbf{{Accuracy}} & {accuracy:.2f} \\\\\\\\\n",
    "        \\\\hline\n",
    "        \\\\end{{tabular}}\n",
    "        \\\\label{{tab:best_classifier}}\n",
    "        \\\\end{{table}}\n",
    "        \"\"\"\n",
    "    with open(f'{k}tw_biclustering_coimbra_test.txt', 'w') as f:\n",
    "        f.write(latex_table)\n",
    "    with open(f'{k}tw_biclusters.txt', 'w') as f:\n",
    "        f.write(bicluster_table.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
